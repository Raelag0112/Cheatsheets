{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cd457d-dd22-40c4-a3e8-8daef9269847",
   "metadata": {},
   "source": [
    "In this tutorial we will override the aggregation method of the GIN convolution module of Pytorch Geometric implementing the following methods:\n",
    "\n",
    "- Principal Neighborhood Aggregation (PNA)\n",
    "- Learning Aggregation Functions (LAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063f3683-3e7d-4c08-8091-93fc35239acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e3e1855fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d8cb1-8b93-498c-a06f-a9ad340110e5",
   "metadata": {},
   "source": [
    "# Message Passing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5332b533-9efe-4bb6-ba79-145ea3626e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79db76aa-2f67-4af9-a7ac-65ed7dd32e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__check_input__',\n",
       " '__class__',\n",
       " '__collect__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lift__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__set_size__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'aggregate',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'edge_update',\n",
       " 'edge_updater',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'jittable',\n",
       " 'load_state_dict',\n",
       " 'message',\n",
       " 'message_and_aggregate',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'propagate',\n",
       " 'register_aggregate_forward_hook',\n",
       " 'register_aggregate_forward_pre_hook',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_edge_update_forward_hook',\n",
       " 'register_edge_update_forward_pre_hook',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_message_and_aggregate_forward_hook',\n",
       " 'register_message_and_aggregate_forward_pre_hook',\n",
       " 'register_message_forward_hook',\n",
       " 'register_message_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'register_propagate_forward_hook',\n",
       " 'register_propagate_forward_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'special_args',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'update',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(MessagePassing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9e4e7-cd15-45de-bbd1-7245086eeca2",
   "metadata": {},
   "source": [
    "We are interested in the <span style='color:Blue'>aggregate</span> method, or, if you are using a sparse adjacency matrix, in the <span style='color:Blue'>message_and_aggregate</span> method. Convolutional classes in PyG extend MessagePassing, we construct our custom convoutional class extending GINConv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf6be9-2974-4acc-abb1-b0454de84452",
   "metadata": {},
   "source": [
    "Scatter operation in <span style='color:Blue'>aggregate</span>:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f5aa-12d3-46df-9c7f-1969a3b1e740",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rusty1s/pytorch_scatter/master/docs/source/_figures/add.svg?sanitize=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f6d1a1-687c-4571-8b6f-772ebef5ba9d",
   "metadata": {},
   "source": [
    "# LAF Aggregation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be89ccce-8a41-4468-a0e8-694133e86725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear\n",
    "from laf_model import LAFLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bbf30-1575-47ac-8461-a30c51e5d8dd",
   "metadata": {},
   "source": [
    "<img src=\"fig/laf.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec46f11-b7b6-470d-a4a9-546f6b48037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLAFConv(GINConv):\n",
    "    def __init__(self, nn, units=1, node_dim=32, **kwargs):\n",
    "        super(GINLAFConv, self).__init__(nn, **kwargs)\n",
    "        self.laf = LAFLayer(units=units, kernel_initializer='random_uniform')\n",
    "        self.mlp = torch.nn.Linear(node_dim*units, node_dim)\n",
    "        self.dim = node_dim\n",
    "        self.units = units\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        x = torch.sigmoid(inputs)\n",
    "        x = self.laf(x, index)\n",
    "        x = x.view((-1, self.dim * self.units))\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8865f0-49c4-419d-a576-303173c91a4c",
   "metadata": {},
   "source": [
    "# PNA Aggregation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb41f1f-96f4-4016-a0e7-94fb18a9c973",
   "metadata": {},
   "source": [
    "<img src=\"fig/pna.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e23a76-bc4d-4fc3-ab31-b17b13fe54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINPNAConv(GINConv):\n",
    "    def __init__(self, nn, node_dim=32, **kwargs):\n",
    "        super(GINPNAConv, self).__init__(nn, **kwargs)\n",
    "        self.mlp = torch.nn.Linear(node_dim*12, node_dim)\n",
    "        self.delta = 2.5749\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        sums = torch_scatter.scatter_add(inputs, index, dim=0)\n",
    "        maxs = torch_scatter.scatter_max(inputs, index, dim=0)[0]\n",
    "        means = torch_scatter.scatter_mean(inputs, index, dim=0)\n",
    "        var = torch.relu(torch_scatter.scatter_mean(inputs ** 2, index, dim=0) - means ** 2)\n",
    "        \n",
    "        aggrs = [sums, maxs, means, var]\n",
    "        c_idx = index.bincount().float().view(-1, 1)\n",
    "        l_idx = torch.log(c_idx + 1.)\n",
    "        \n",
    "        amplification_scaler = [c_idx / self.delta * a for a in aggrs]\n",
    "        attenuation_scaler = [self.delta / c_idx * a for a in aggrs]\n",
    "        combinations = torch.cat(aggrs+ amplification_scaler+ attenuation_scaler, dim=1)\n",
    "        x = self.mlp(combinations)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d878bc-34ae-443d-b418-222f6ec6b3ed",
   "metadata": {},
   "source": [
    "# Test the new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567b2080-cfb7-49c1-b76e-1bfd940fb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing, SAGEConv, GINConv, global_add_pool\n",
    "import torch_scatter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51d16e97-30d8-4488-836f-1b05638ed9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Extracting data\\TU\\MUTAG\\MUTAG.zip\n",
      "Processing...\n",
      "Done!\n",
      "C:\\Users\\Travail\\anaconda3\\envs\\pyg\\lib\\site-packages\\torch_geometric\\deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "path = osp.join('./', 'data', 'TU')\n",
    "dataset = TUDataset(path, name='MUTAG').shuffle()\n",
    "test_dataset = dataset[:len(dataset) // 10]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b6b34b-7755-42b6-8083-67b9c49bbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LAFNet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "        units = 3\n",
    "        \n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINLAFConv(nn1, units=units, node_dim=num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINLAFConv(nn2, units=units, node_dim=dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINLAFConv(nn3, units=units, node_dim=dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINLAFConv(nn4, units=units, node_dim=dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINLAFConv(nn5, units=units, node_dim=dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33efa67f-b23e-4d9b-bcc8-2d2342d685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNANet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "\n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINPNAConv(nn1, node_dim=num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINPNAConv(nn2, node_dim=dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINPNAConv(nn3, node_dim=dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINPNAConv(nn4, node_dim=dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINPNAConv(nn5, node_dim=dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6625f06-f603-4770-971b-ceb647308eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GINNet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "\n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da9e0592-3956-4ab8-977c-632ca482062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.3497609, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 002, Train Loss: 1.0218635, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 003, Train Loss: 0.8216159, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 004, Train Loss: 0.6225654, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 005, Train Loss: 0.6029876, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 006, Train Loss: 0.4300755, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 007, Train Loss: 0.4750554, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 008, Train Loss: 0.4779935, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 009, Train Loss: 0.3871457, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 010, Train Loss: 0.4097661, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 011, Train Loss: 0.3643195, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 012, Train Loss: 0.3774190, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 013, Train Loss: 0.3398798, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 014, Train Loss: 0.3115931, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 015, Train Loss: 0.2790069, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 016, Train Loss: 0.3423936, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 017, Train Loss: 0.2482179, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 018, Train Loss: 0.2469805, Train Acc: 0.6705882, Test Acc: 0.7222222\n",
      "Epoch: 019, Train Loss: 0.2096540, Train Acc: 0.7000000, Test Acc: 0.7222222\n",
      "Epoch: 020, Train Loss: 0.2224759, Train Acc: 0.7176471, Test Acc: 0.8333333\n",
      "Epoch: 021, Train Loss: 0.2156863, Train Acc: 0.7235294, Test Acc: 0.9444444\n",
      "Epoch: 022, Train Loss: 0.1987324, Train Acc: 0.7470588, Test Acc: 0.8333333\n",
      "Epoch: 023, Train Loss: 0.1898772, Train Acc: 0.7882353, Test Acc: 0.8333333\n",
      "Epoch: 024, Train Loss: 0.1881019, Train Acc: 0.8000000, Test Acc: 0.8333333\n",
      "Epoch: 025, Train Loss: 0.1756682, Train Acc: 0.8117647, Test Acc: 0.8888889\n",
      "Epoch: 026, Train Loss: 0.1778698, Train Acc: 0.7882353, Test Acc: 0.8333333\n",
      "Epoch: 027, Train Loss: 0.1247382, Train Acc: 0.7705882, Test Acc: 0.7777778\n",
      "Epoch: 028, Train Loss: 0.1473643, Train Acc: 0.7176471, Test Acc: 0.7777778\n",
      "Epoch: 029, Train Loss: 0.1461425, Train Acc: 0.8058824, Test Acc: 0.8333333\n",
      "Epoch: 030, Train Loss: 0.1435242, Train Acc: 0.8470588, Test Acc: 0.8333333\n",
      "Epoch: 031, Train Loss: 0.1628855, Train Acc: 0.8764706, Test Acc: 0.8888889\n",
      "Epoch: 032, Train Loss: 0.1610416, Train Acc: 0.8764706, Test Acc: 0.8333333\n",
      "Epoch: 033, Train Loss: 0.1160962, Train Acc: 0.8588235, Test Acc: 0.8888889\n",
      "Epoch: 034, Train Loss: 0.1159927, Train Acc: 0.8882353, Test Acc: 0.8333333\n",
      "Epoch: 035, Train Loss: 0.2466172, Train Acc: 0.6176471, Test Acc: 0.6111111\n",
      "Epoch: 036, Train Loss: 0.1972225, Train Acc: 0.3705882, Test Acc: 0.3888889\n",
      "Epoch: 037, Train Loss: 0.1595776, Train Acc: 0.4000000, Test Acc: 0.4444444\n",
      "Epoch: 038, Train Loss: 0.1758044, Train Acc: 0.4705882, Test Acc: 0.5000000\n",
      "Epoch: 039, Train Loss: 0.1634223, Train Acc: 0.5235294, Test Acc: 0.5000000\n",
      "Epoch: 040, Train Loss: 0.1464215, Train Acc: 0.7058824, Test Acc: 0.6666667\n",
      "Epoch: 041, Train Loss: 0.1476715, Train Acc: 0.8117647, Test Acc: 0.6666667\n",
      "Epoch: 042, Train Loss: 0.1206777, Train Acc: 0.9294118, Test Acc: 0.8888889\n",
      "Epoch: 043, Train Loss: 0.1351910, Train Acc: 0.9411765, Test Acc: 0.8333333\n",
      "Epoch: 044, Train Loss: 0.0966650, Train Acc: 0.9411765, Test Acc: 0.7777778\n",
      "Epoch: 045, Train Loss: 0.1103115, Train Acc: 0.9235294, Test Acc: 0.8333333\n",
      "Epoch: 046, Train Loss: 0.0824212, Train Acc: 0.9294118, Test Acc: 0.8333333\n",
      "Epoch: 047, Train Loss: 0.1197868, Train Acc: 0.9294118, Test Acc: 0.8888889\n",
      "Epoch: 048, Train Loss: 0.0893536, Train Acc: 0.9352941, Test Acc: 0.7777778\n",
      "Epoch: 049, Train Loss: 0.0799070, Train Acc: 0.9000000, Test Acc: 0.7222222\n",
      "Epoch: 050, Train Loss: 0.1145215, Train Acc: 0.9235294, Test Acc: 0.7777778\n",
      "Epoch: 051, Train Loss: 0.0947837, Train Acc: 0.9352941, Test Acc: 0.7777778\n",
      "Epoch: 052, Train Loss: 0.0640838, Train Acc: 0.9411765, Test Acc: 0.7777778\n",
      "Epoch: 053, Train Loss: 0.0741564, Train Acc: 0.9470588, Test Acc: 0.7777778\n",
      "Epoch: 054, Train Loss: 0.0871304, Train Acc: 0.9529412, Test Acc: 0.7777778\n",
      "Epoch: 055, Train Loss: 0.0487207, Train Acc: 0.9470588, Test Acc: 0.7777778\n",
      "Epoch: 056, Train Loss: 0.0637015, Train Acc: 0.9470588, Test Acc: 0.7777778\n",
      "Epoch: 057, Train Loss: 0.0693471, Train Acc: 0.9411765, Test Acc: 0.8333333\n",
      "Epoch: 058, Train Loss: 0.0575097, Train Acc: 0.9411765, Test Acc: 0.8333333\n",
      "Epoch: 059, Train Loss: 0.0502624, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 060, Train Loss: 0.0482633, Train Acc: 0.9764706, Test Acc: 0.9444444\n",
      "Epoch: 061, Train Loss: 0.0526239, Train Acc: 0.9764706, Test Acc: 0.9444444\n",
      "Epoch: 062, Train Loss: 0.0498621, Train Acc: 0.9647059, Test Acc: 0.9444444\n",
      "Epoch: 063, Train Loss: 0.0453484, Train Acc: 0.9647059, Test Acc: 0.9444444\n",
      "Epoch: 064, Train Loss: 0.0598439, Train Acc: 0.9647059, Test Acc: 0.9444444\n",
      "Epoch: 065, Train Loss: 0.0385793, Train Acc: 0.9588235, Test Acc: 0.8888889\n",
      "Epoch: 066, Train Loss: 0.0582834, Train Acc: 0.9588235, Test Acc: 0.8333333\n",
      "Epoch: 067, Train Loss: 0.0496932, Train Acc: 0.9647059, Test Acc: 0.8333333\n",
      "Epoch: 068, Train Loss: 0.0244162, Train Acc: 0.9647059, Test Acc: 0.8333333\n",
      "Epoch: 069, Train Loss: 0.0200977, Train Acc: 0.9529412, Test Acc: 0.8888889\n",
      "Epoch: 070, Train Loss: 0.0286441, Train Acc: 0.9529412, Test Acc: 0.9444444\n",
      "Epoch: 071, Train Loss: 0.0372349, Train Acc: 0.9529412, Test Acc: 0.9444444\n",
      "Epoch: 072, Train Loss: 0.0392289, Train Acc: 0.9529412, Test Acc: 0.9444444\n",
      "Epoch: 073, Train Loss: 0.0314294, Train Acc: 0.9647059, Test Acc: 0.8888889\n",
      "Epoch: 074, Train Loss: 0.0328549, Train Acc: 0.9705882, Test Acc: 0.8888889\n",
      "Epoch: 075, Train Loss: 0.0228421, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 076, Train Loss: 0.0305997, Train Acc: 0.9647059, Test Acc: 0.8333333\n",
      "Epoch: 077, Train Loss: 0.0253990, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 078, Train Loss: 0.0306646, Train Acc: 0.9705882, Test Acc: 0.8888889\n",
      "Epoch: 079, Train Loss: 0.0164476, Train Acc: 0.9705882, Test Acc: 0.8888889\n",
      "Epoch: 080, Train Loss: 0.0184595, Train Acc: 0.9647059, Test Acc: 0.9444444\n",
      "Epoch: 081, Train Loss: 0.0272709, Train Acc: 0.9647059, Test Acc: 0.9444444\n",
      "Epoch: 082, Train Loss: 0.0196232, Train Acc: 0.9705882, Test Acc: 0.9444444\n",
      "Epoch: 083, Train Loss: 0.0284858, Train Acc: 0.9705882, Test Acc: 0.9444444\n",
      "Epoch: 084, Train Loss: 0.0241170, Train Acc: 0.9705882, Test Acc: 0.8888889\n",
      "Epoch: 085, Train Loss: 0.0288804, Train Acc: 0.9647059, Test Acc: 0.8888889\n",
      "Epoch: 086, Train Loss: 0.0242770, Train Acc: 0.9705882, Test Acc: 0.8888889\n",
      "Epoch: 087, Train Loss: 0.0217948, Train Acc: 0.9647059, Test Acc: 0.8888889\n",
      "Epoch: 088, Train Loss: 0.0215536, Train Acc: 0.9588235, Test Acc: 0.8333333\n",
      "Epoch: 089, Train Loss: 0.0179766, Train Acc: 0.9470588, Test Acc: 0.8333333\n",
      "Epoch: 090, Train Loss: 0.0309706, Train Acc: 0.9411765, Test Acc: 0.8333333\n",
      "Epoch: 091, Train Loss: 0.0251052, Train Acc: 0.9588235, Test Acc: 0.8888889\n",
      "Epoch: 092, Train Loss: 0.0341602, Train Acc: 0.9588235, Test Acc: 0.8333333\n",
      "Epoch: 093, Train Loss: 0.0186183, Train Acc: 0.9647059, Test Acc: 0.8333333\n",
      "Epoch: 094, Train Loss: 0.0143378, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 095, Train Loss: 0.0205141, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 096, Train Loss: 0.0141937, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 097, Train Loss: 0.0161137, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 098, Train Loss: 0.0155163, Train Acc: 0.9705882, Test Acc: 0.8333333\n",
      "Epoch: 099, Train Loss: 0.0249196, Train Acc: 0.9705882, Test Acc: 0.8888889\n",
      "Epoch: 100, Train Loss: 0.0195986, Train Acc: 0.9705882, Test Acc: 0.8888889\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = \"PNA\"\n",
    "if net == \"LAF\":\n",
    "    model = LAFNet().to(device)\n",
    "elif net == \"PNA\":\n",
    "    model = PNANet().to(device)\n",
    "elif net == \"GIN\":\n",
    "    GINNet().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    train_loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
    "          'Train Acc: {:.7f}, Test Acc: {:.7f}'.format(epoch, train_loss,\n",
    "                                                       train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
